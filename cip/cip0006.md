---
author: "lawrennd"
created: "2025-11-06"
id: "0006"
last_updated: "2025-11-06"
status: proposed
tags:
- cip
- llm
- langchain
- compute
- ai-integration
title: "LLM Integration for Compute Framework"
---

# CIP-0006: LLM Integration for Compute Framework

## Summary

Extend the referia compute framework with Large Language Model (LLM) capabilities by integrating LangChain or similar frameworks. This will enable AI-powered text analysis, generation, summarisation, and transformation operations within the existing declarative compute specification system.

**Architectural Decision**: This implementation will start in referia (application layer) to enable rapid iteration and domain-specific development. Once the patterns are proven and stabilised, general-purpose components may be promoted to lynguine (infrastructure layer) for use by other applications. This follows the same evolutionary path as existing text processing functions like `text_summarizer` and `word_count`.

## Motivation

### Current Limitations

The compute framework currently provides excellent text processing functions (word_count, text_summarizer, named_entities, etc.), but these are primarily rule-based or classical NLP approaches. Modern LLMs offer significantly more powerful and flexible capabilities for:

1. **Semantic Analysis**: Understanding context, sentiment, and intent beyond simple pattern matching
2. **Content Generation**: Creating summaries, responses, or transformations based on complex instructions
3. **Question Answering**: Extracting specific information from text based on natural language queries
4. **Classification**: Categorizing content with nuanced understanding of context
5. **Extraction**: Pulling structured data from unstructured text
6. **Translation**: Converting between languages or formats
7. **Review and Assessment**: Automated feedback generation, scoring, and evaluation

### Use Cases in Referia

For review and assessment workflows, LLMs can:

- **Automated Review Summarisation**: Generate concise summaries of long reviews
- **Sentiment Analysis**: Understand reviewer sentiment beyond simple positive/negative
- **Quality Assessment**: Evaluate review quality and completeness
- **Response Generation**: Draft responses to reviews or queries
- **Information Extraction**: Pull key points, recommendations, or action items from reviews
- **Comparative Analysis**: Compare multiple reviews and identify consensus or conflicts
- **Rubric-Based Scoring**: Apply scoring rubrics with contextual understanding

### Why Now?

1. **Mature Ecosystem**: LangChain and similar frameworks provide stable, production-ready LLM integration
2. **Cost Reduction**: LLM API costs have decreased significantly, making production use viable
3. **Existing Architecture**: Our compute framework is perfectly positioned to integrate LLMs as functions
4. **User Demand**: Assessment workflows increasingly need AI-powered analysis capabilities

## Architectural Strategy

### Implementation Location: Referia First

This CIP implements LLM integration in **referia** (application layer) rather than lynguine (infrastructure layer). This decision is based on:

1. **Rapid Iteration**: Assessment/review workflows are the primary use case, and referia allows faster experimentation
2. **Domain Specificity**: Initial prompts and functions will be review-focused
3. **Proven Pattern**: Follows the same path as `text_summarizer`, `word_count`, and other NLP functions
4. **Risk Management**: Easier to refactor at application layer than infrastructure layer

### Future Migration to Lynguine

Once patterns are proven, we will identify general-purpose components to promote to lynguine:

**Candidates for Promotion:**
- Core `LLMManager` class (connection management, retry, caching)
- Generic functions: `llm_complete`, `llm_chat`, `llm_extract`, `llm_classify`
- Configuration schema for LLM settings
- Cost tracking and monitoring infrastructure

**Staying in Referia:**
- Review-specific prompts and templates
- Assessment-focused scoring functions
- Domain-specific prompt libraries
- Referia-specific integrations (e.g., with PDF processing)

**Migration Trigger:**
When other lynguine-based applications (beyond referia) need LLM capabilities, we'll create a follow-up CIP to extract and promote the generic components to lynguine.

### Code Organization

```
referia/
├── util/
│   ├── llm.py              # LLMManager, connection handling
│   └── llm_prompts.py      # Review-specific prompt templates
├── assess/
│   └── compute.py          # LLM compute functions in _compute_functions_list()
└── config/
    └── interface.py        # Extended with 'llm' config section
```

## Detailed Description

### Architecture Overview

The LLM integration will follow the existing compute function pattern:

```yaml
compute:
  - function: llm_summarise
    field: ai_summary
    row_args:
      text: review_text
    args:
      model: "gpt-4o-mini"
      max_tokens: 150
      temperature: 0.3
```

### Core Components

#### 1. LLM Connection Manager

A singleton class that manages LLM connections and configuration:

```python
class LLMManager:
    """Manage LLM connections and configuration."""
    
    def __init__(self, config):
        self.providers = {}  # Cache of provider connections
        self.config = config
        
    def get_client(self, provider="openai", model="gpt-4o-mini"):
        """Get or create LLM client."""
        pass
        
    def call(self, prompt, model, **kwargs):
        """Execute LLM call with retry logic and error handling."""
        pass
```

#### 2. LLM Compute Functions

New functions to add to the compute registry:

**Basic LLM Functions:**
- `llm_complete`: General-purpose text completion
- `llm_chat`: Chat-based interaction with conversation history
- `llm_summarise`: Specialised summarisation
- `llm_extract`: Extract structured information
- `llm_classify`: Classify text into categories
- `llm_translate`: Language translation
- `llm_score`: Score text against criteria

**Advanced LLM Functions:**
- `llm_chain`: Execute multi-step LLM chains
- `llm_prompt_template`: Use templated prompts with variable substitution
- `llm_with_context`: Include additional context from other columns
- `llm_batch`: Batch multiple prompts efficiently

#### 3. Configuration System

Extend the interface configuration to support LLM settings:

```yaml
llm:
  default_provider: "openai"
  default_model: "gpt-4o-mini"
  api_keys:
    openai: ${OPENAI_API_KEY}
    anthropic: ${ANTHROPIC_API_KEY}
  rate_limits:
    requests_per_minute: 60
    tokens_per_minute: 150000
  retry:
    max_attempts: 3
    backoff_factor: 2
  cache:
    enabled: true
    ttl: 3600

compute:
  - function: llm_summarise
    field: summary
    row_args:
      text: review_text
    args:
      model: "gpt-4o-mini"
      temperature: 0.3
      system_prompt: "You are an expert at summarising academic reviews."
```

#### 4. Prompt Management

Support for prompt templates and prompt engineering:

```yaml
prompts:
  review_summary:
    template: |
      Summarise the following academic review in 2-3 sentences.
      Focus on the main strengths, weaknesses, and recommendation.
      
      Review:
      {{ review_text }}
    model: "gpt-4o-mini"
    temperature: 0.3
    max_tokens: 150

compute:
  - function: llm_prompt_template
    field: summary
    row_args:
      review_text: review_text
    args:
      prompt_name: "review_summary"
```

### Integration with LangChain

We'll use LangChain as the primary framework because:

1. **Comprehensive**: Supports multiple LLM providers (OpenAI, Anthropic, local models)
2. **Production-Ready**: Mature error handling, retry logic, and monitoring
3. **Feature-Rich**: Chains, agents, memory, and structured output
4. **Active Development**: Regular updates and community support

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

def llm_summarise(text, model="gpt-4o-mini", temperature=0.3, max_tokens=150, system_prompt=None):
    """Summarise text using an LLM."""
    llm = ChatOpenAI(model=model, temperature=temperature, max_tokens=max_tokens)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt or "You are a helpful assistant that summarizes text."),
        ("human", "Summarize the following text:\n\n{text}")
    ])
    
    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.run(text=text)
    
    return result
```

### Error Handling and Resilience

1. **Rate Limiting**: Respect API rate limits with automatic backoff
2. **Retries**: Automatic retry with exponential backoff for transient errors
3. **Fallbacks**: Option to fall back to simpler functions if LLM fails
4. **Caching**: Cache LLM responses to reduce costs and improve performance
5. **Validation**: Validate LLM outputs before returning

```yaml
compute:
  - function: llm_summarise
    field: summary
    row_args:
      text: review_text
    args:
      model: "gpt-4o-mini"
      retry:
        max_attempts: 3
        backoff: 2
      fallback_function: text_summariser  # Use existing function if LLM fails
      cache_ttl: 3600
      validate: true
```

### Cost Management

1. **Token Counting**: Track and log token usage
2. **Budgets**: Set per-run or per-month budgets
3. **Model Selection**: Easy switching between expensive and cheap models
4. **Batching**: Batch multiple prompts to reduce overhead
5. **Caching**: Aggressive caching to avoid redundant calls

```python
class CostTracker:
    """Track LLM costs and enforce budgets."""
    
    def __init__(self, budget_per_run=None, budget_per_month=None):
        self.total_tokens = 0
        self.total_cost = 0.0
        self.budget_per_run = budget_per_run
        
    def log_call(self, model, input_tokens, output_tokens):
        """Log an LLM call and check budgets."""
        cost = self._calculate_cost(model, input_tokens, output_tokens)
        self.total_cost += cost
        
        if self.budget_per_run and self.total_cost > self.budget_per_run:
            raise BudgetExceededError(f"Run budget exceeded: ${self.total_cost:.2f}")
```

## Implementation Plan

### Phase 1: Core Infrastructure (Week 1-2)

1. **Add Dependencies**:
   - [ ] Add langchain to pyproject.toml
   - [ ] Add openai, anthropic, and other provider SDKs
   - [ ] Add tenacity for retry logic
   - [ ] Add diskcache or redis for caching

2. **Create LLM Manager**:
   - [ ] Implement `referia/util/llm.py` with `LLMManager` class
   - [ ] Support OpenAI and Anthropic initially
   - [ ] Implement retry logic with exponential backoff
   - [ ] Add connection pooling and resource management

3. **Configuration System**:
   - [ ] Extend Interface to support `llm` configuration section
   - [ ] Environment variable support for API keys
   - [ ] Validation of LLM configuration
   - [ ] Default configuration for common use cases

### Phase 2: Basic LLM Functions (Week 3-4)

1. **Implement Core Functions**:
   - [ ] `llm_complete`: Basic text completion
   - [ ] `llm_chat`: Chat-based interaction
   - [ ] `llm_summarise`: Specialised summarisation
   - [ ] `llm_extract`: Extract structured data
   - [ ] `llm_classify`: Text classification

2. **Register Functions**:
   - [ ] Add functions to `_compute_functions_list()` in `referia/assess/compute.py`
   - [ ] Define appropriate default arguments
   - [ ] Add comprehensive docstrings
   - [ ] Set `context: True` where needed

3. **Error Handling**:
   - [ ] Implement retry logic for transient errors
   - [ ] Graceful degradation when LLM unavailable
   - [ ] Detailed error messages and logging
   - [ ] Fallback to classical functions when configured

### Phase 3: Advanced Features (Week 5-6)

1. **Prompt Management**:
   - [ ] Support prompt templates in configuration
   - [ ] Liquid template integration for prompts
   - [ ] Prompt versioning and management
   - [ ] Few-shot prompt examples

2. **Caching System**:
   - [ ] Implement response caching with TTL
   - [ ] Cache key generation (hash of prompt + args)
   - [ ] Cache invalidation strategies
   - [ ] Statistics on cache hit rates

3. **Cost Management**:
   - [ ] Token counting and logging
   - [ ] Cost calculation per model
   - [ ] Budget enforcement
   - [ ] Usage reporting and dashboards

4. **Advanced Functions**:
   - [ ] `llm_chain`: Multi-step LLM chains
   - [ ] `llm_with_context`: Include context from other columns
   - [ ] `llm_batch`: Batch processing for efficiency
   - [ ] `llm_agent`: Agent-based reasoning

### Phase 4: Documentation and Testing (Week 7-8)

1. **Documentation**:
   - [ ] Add LLM section to compute framework documentation
   - [ ] Create tutorial for common use cases
   - [ ] Document prompt engineering best practices
   - [ ] Add API reference for all LLM functions
   - [ ] Create cost estimation guide

2. **Testing**:
   - [ ] Unit tests for all LLM functions (with mocking)
   - [ ] Integration tests with real LLM APIs (optional, CI-skipped)
   - [ ] Test retry and error handling
   - [ ] Test caching behavior
   - [ ] Test cost tracking and budgets
   - [ ] Performance benchmarks

3. **Examples**:
   - [ ] Create example configurations for common workflows
   - [ ] Add notebook demonstrating LLM integration
   - [ ] Create sample prompts library
   - [ ] Add case studies from real usage

### Phase 5: Production Readiness (Week 9-10)

1. **Monitoring**:
   - [ ] Add metrics for LLM calls (latency, errors, costs)
   - [ ] Integration with existing logging
   - [ ] Alerting for budget overruns or high error rates
   - [ ] Dashboard for LLM usage statistics

2. **Security**:
   - [ ] Secure API key management
   - [ ] Input sanitisation to prevent prompt injection
   - [ ] Output validation and filtering
   - [ ] Audit logging for LLM calls

3. **Performance Optimisation**:
   - [ ] Parallel execution of independent LLM calls
   - [ ] Streaming responses for long outputs
   - [ ] Efficient batching strategies
   - [ ] Connection pooling and reuse

## Backward Compatibility

This is a **fully backward compatible** addition to referia:

- **No Breaking Changes**: All existing compute functions in both lynguine and referia continue to work
- **Optional Dependency**: LangChain and LLM providers are optional dependencies for referia
- **Graceful Degradation**: Referia works without LLM configuration
- **No Required Changes**: Existing configurations don't need modification
- **Lynguine Unaffected**: No changes to lynguine in this CIP

Users opt-in by:
1. Installing LLM dependencies in referia: `poetry add langchain openai anthropic`
2. Configuring API keys in environment or configuration
3. Adding LLM functions to their referia compute specifications

## Testing Strategy

### Unit Tests

```python
# tests/test_llm_functions.py
def test_llm_summarise_with_mock():
    """Test LLM summarisation with mocked API."""
    with patch('langchain.chat_models.ChatOpenAI') as mock_llm:
        mock_llm.return_value.predict.return_value = "Test summary"
        result = llm_summarise("Long text here")
        assert result == "Test summary"

def test_llm_retry_on_rate_limit():
    """Test retry logic for rate limits."""
    # Test that rate limit errors trigger retries
    pass

def test_llm_fallback_on_error():
    """Test fallback to classical function on LLM error."""
    # Test that fallback_function is called when LLM fails
    pass
```

### Integration Tests (Optional)

```python
# tests/integration/test_llm_integration.py
@pytest.mark.integration
@pytest.mark.skipif(not os.getenv('OPENAI_API_KEY'), reason="API key not available")
def test_real_llm_call():
    """Test actual LLM API call (requires API key)."""
    result = llm_summarise("This is a test text.")
    assert len(result) > 0
    assert isinstance(result, str)
```

### Performance Tests

```python
def test_llm_caching():
    """Test that repeated calls use cache."""
    # First call should hit API
    result1 = llm_summarise("Test text")
    # Second call should use cache
    result2 = llm_summarise("Test text")
    assert result1 == result2
    # Verify cache was used (check logs or metrics)
```

## Cost Considerations

### Token Usage Estimates

For a typical review workflow processing 100 reviews:

- **Input**: 100 reviews × 500 words each × 1.3 tokens/word = ~65,000 tokens
- **Output**: 100 summaries × 50 words each × 1.3 tokens/word = ~6,500 tokens
- **Total**: ~71,500 tokens

**Cost Estimates (as of 2025-11-06):**

- **GPT-4o-mini**: $0.15/1M input + $0.60/1M output = ~$0.015 per run
- **GPT-4o**: $2.50/1M input + $10.00/1M output = ~$0.23 per run
- **Claude 3 Haiku**: $0.25/1M input + $1.25/1M output = ~$0.025 per run

### Cost Mitigation Strategies

1. **Caching**: Cache responses for identical inputs (could save 50-90%)
2. **Batch Processing**: Process multiple items in single API call when possible
3. **Model Selection**: Use cheaper models (mini/haiku) for simple tasks
4. **Smart Refresh**: Only recompute when input changes (leverage compute `refresh: false`)
5. **Budgets**: Enforce spending limits to prevent runaway costs

## Related Requirements

This CIP addresses the following needs:

- **AI-Powered Analysis**: Enable modern AI capabilities in assessment workflows
- **Extensibility**: Maintain the compute framework's extensibility and ease of use
- **Production Readiness**: Enterprise-grade error handling, monitoring, and cost control
- **User Experience**: Simple declarative interface for powerful AI capabilities

Specifically, it implements solutions for:
- Automated review summarization and analysis
- Intelligent text extraction and classification
- Natural language understanding in assessment contexts
- Scalable AI integration with cost controls
- Flexible prompt engineering and model selection

## Implementation Status

### Phase 1: Core Infrastructure
- [ ] Add dependencies (langchain, openai, anthropic, tenacity, diskcache)
- [ ] Create LLMManager class
- [ ] Configuration system for LLM settings
- [ ] API key management from environment

### Phase 2: Basic LLM Functions
- [ ] Implement llm_complete function
- [ ] Implement llm_chat function
- [ ] Implement llm_summarize function
- [ ] Implement llm_extract function
- [ ] Implement llm_classify function
- [ ] Register functions in compute registry
- [ ] Error handling and retries

### Phase 3: Advanced Features
- [ ] Prompt template system
- [ ] Response caching with TTL
- [ ] Cost tracking and budgets
- [ ] llm_chain for multi-step processing
- [ ] llm_batch for efficient batching

### Phase 4: Documentation and Testing
- [ ] Documentation for LLM functions
- [ ] Tutorial for common use cases
- [ ] Unit tests with mocking
- [ ] Integration tests (optional)
- [ ] Example configurations and notebooks

### Phase 5: Production Readiness
- [ ] Monitoring and metrics
- [ ] Security hardening
- [ ] Performance optimization
- [ ] Production deployment guide

## References

### Documentation
- [LangChain Documentation](https://python.langchain.com/)
- [OpenAI API Reference](https://platform.openai.com/docs/)
- [Anthropic Claude API](https://docs.anthropic.com/)
- [Compute Framework Documentation](../lynguine/docs/compute_framework.md)

### Related Code (Referia)
- `referia/assess/compute.py`: Compute class and function registry (where LLM functions will be added)
- `referia/util/text.py`: Existing text processing functions (pattern to follow)
- `referia/util/llm.py`: New LLM utilities (to be created)
- `referia/config/interface.py`: Interface configuration (to be extended with LLM config)

### Related Code (Lynguine)
- `lynguine/assess/compute.py`: Base compute class (inherited by referia)
- `lynguine/docs/compute_framework.md`: Compute framework documentation

### Similar Projects
- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)
- [Haystack](https://haystack.deepset.ai/): Alternative LLM framework
- [LlamaIndex](https://www.llamaindex.ai/): For RAG applications

### Best Practices
- [OpenAI Best Practices](https://platform.openai.com/docs/guides/production-best-practices)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [LangChain Production Guide](https://python.langchain.com/docs/guides/productionization/)

## Security Considerations

1. **API Key Management**: Never commit API keys; use environment variables
2. **Input Sanitisation**: Validate and sanitise text inputs to prevent prompt injection
3. **Output Filtering**: Validate LLM outputs before storing in data
4. **Rate Limiting**: Respect API rate limits to avoid account suspension
5. **Audit Logging**: Log all LLM calls for security and debugging
6. **Data Privacy**: Ensure sensitive data handling complies with policies

## Future Enhancements

Beyond this initial implementation, future CIPs could address:

1. **Local LLM Support**: Integration with local models (Llama, Mistral) for privacy
2. **RAG Integration**: Retrieval-Augmented Generation for knowledge-based responses
3. **Fine-Tuning**: Support for fine-tuned models on domain-specific data
4. **Multi-Modal**: Support for image, audio, and video processing
5. **Agent Frameworks**: More sophisticated agent-based reasoning
6. **Evaluation Framework**: Automated evaluation of LLM outputs
7. **A/B Testing**: Compare outputs from different models or prompts
8. **Streaming**: Real-time streaming of LLM responses

## Questions and Discussion

### Open Questions

1. Should we support multiple LLM frameworks (LangChain, LlamaIndex, direct APIs)?
2. What's the best approach for prompt versioning and management?
3. Should we implement a GUI for prompt engineering?
4. How should we handle very long contexts that exceed token limits?
5. What metrics should we expose for LLM performance monitoring?

### Discussion Points

- **Model Selection**: Should we have a recommended default model?
- **Pricing**: Should we build a cost estimator before runs?
- **Local Models**: Priority for local model support?
- **Evaluation**: How to evaluate LLM output quality automatically?

## Appendix: Example Configurations

### Example 1: Simple Summarization

```yaml
llm:
  default_provider: openai
  default_model: gpt-4o-mini
  api_keys:
    openai: ${OPENAI_API_KEY}

compute:
  - function: llm_summarise
    field: summary
    row_args:
      text: review_text
    args:
      max_tokens: 150
      temperature: 0.3
```

### Example 2: Multi-Step Chain

```yaml
compute:
  # Step 1: Extract key points
  - function: llm_extract
    field: key_points
    row_args:
      text: review_text
    args:
      system_prompt: "Extract 3-5 key points from the review as a bullet list."
  
  # Step 2: Generate response
  - function: llm_complete
    field: response_draft
    row_args:
      key_points: key_points
    args:
      system_prompt: "Draft a response addressing these key points from the review."
      temperature: 0.7
```

### Example 3: Classification with Fallback

```yaml
compute:
  - function: llm_classify
    field: sentiment
    row_args:
      text: review_text
    args:
      categories: ["positive", "negative", "neutral"]
      model: gpt-4o-mini
      fallback_function: simple_sentiment  # Use rule-based if LLM fails
      cache_ttl: 86400  # Cache for 24 hours
```

### Example 4: Batch Processing

```yaml
compute:
  - function: llm_batch
    field: summaries
    column_args:
      texts: review_texts
    args:
      operation: summarize
      batch_size: 10
      model: gpt-4o-mini
      max_tokens: 100
```

